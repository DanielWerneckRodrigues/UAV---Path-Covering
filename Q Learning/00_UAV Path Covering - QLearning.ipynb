{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "from multiprocessing.pool import Pool\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definindo cenários"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define 3 matrizes para problema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz Extra Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = [0, 0, 0]\n",
    "row_2 = [0, 0, 0]\n",
    "row_3 = [0, -100, 0.5]\n",
    "row_4 = [0, 0.5, 0.9]\n",
    "\n",
    "matrix_extra_simple = np.matrix([row_1, row_2, row_3, row_4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = [0, 0, 0, 0, 0]\n",
    "row_2 = [0, 0, 0, 0, 0]\n",
    "row_3 = [0, 0, -100, 0.5, 0.5]\n",
    "row_4 = [0, 0, 0.5, 0.9, 0.9]\n",
    "row_5 = [0, 0, 0.5, 0.9, 0.9]\n",
    "\n",
    "matrix_simple = np.matrix([row_1, row_2, row_3, row_4, row_5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz Média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = [0, 0, 0, 0, 0, 0.5, 0.5, 0.5]\n",
    "row_2 = [0, 0, 0, 0, 0, 0.5, 0.6, 0.6]\n",
    "row_3 = [0, 0, 0, 0, 0, -100, 0.6, 0.7]\n",
    "row_4 = [0, 0, -100, 0, 0, 0.5, 0.6, 0.7]\n",
    "row_5 = [0, 0, 0, 0, 0, 0.5, 0.9, 0.9]\n",
    "row_6 = [0, 0, 0, 0, 0, 0.5, 0.9, 0.9]\n",
    "row_7 = [0, 0, 0, 0, -100, 0.5, 0.9, 0.9]\n",
    "row_8 = [0, -100, 0, 0, 0, 0.5, 0.9, 0.9]\n",
    "\n",
    "matrix_medium = np.matrix([row_1, row_2, row_3, row_4, row_5, row_6, row_7, row_8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz Complexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = [0, 0, 0, 0, 0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "row_2 = [0, 0, 0, 0, 0, 0.5, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]\n",
    "row_3 = [0, 0, 0, 0, 0, -100, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7]\n",
    "row_4 = [0, 0, -100, 0, 0, 0.5, 0.6, 0.7, 0.8, -100, 0.8, 0.8, 0.8]\n",
    "row_5 = [0, 0, 0, 0, 0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9, 0.9, 0.8]\n",
    "row_6 = [0, 0, 0, 0, 0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9, 0.9, 0.8]\n",
    "row_7 = [0, 0, 0, 0, -100, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9, 0.9, 0.8]\n",
    "row_8 = [0, -100, 0, 0, 0, 0.5, 0.6, 0.7, -100, 0.8, 0.8, 0.8, 0.8]\n",
    "row_9 = [0, 0, 0, 0, 0, 0.5, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7]\n",
    "\n",
    "matrix_complex = np.matrix([row_1, row_2, row_3, row_4, row_5, row_6, row_7, row_8, row_9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Plota Matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7))\n",
    "\n",
    "# Assign heatmaps to each subplot using sns.heatmap\n",
    "sns.heatmap(matrix_extra_simple, ax=axs[0, 0], vmin=-1, vmax=1, annot=True, fmt='g')\n",
    "sns.heatmap(matrix_simple, ax=axs[0, 1], vmin=-1, vmax=1, annot=True, fmt='g')\n",
    "sns.heatmap(matrix_medium, ax=axs[1, 0], vmin=-1, vmax=1, annot=True, fmt='g')\n",
    "sns.heatmap(matrix_complex, ax=axs[1, 1], vmin=-1, vmax=1, annot=True, fmt='g')\n",
    "\n",
    "# Set titles for each subplot\n",
    "axs[0, 0].set_title('Matrix_extra_simple (4x3)')\n",
    "axs[0, 1].set_title('Matrix_simple (5x5)')\n",
    "axs[1, 0].set_title('Matrix_medium (8x8)')\n",
    "axs[1, 1].set_title('Matrix_complex (9x13)')\n",
    "\n",
    "# Modify the colorbar ticks\n",
    "# cbar = ax.collections[0].colorbar\n",
    "# cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Classes de Estado e Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATRIX_GAME = matrix_simple\\nSTART = (0, 0)\\nDETERMINISTIC = True'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global variables\n",
    "'''MATRIX_GAME = matrix_simple\n",
    "START = (0, 0)\n",
    "DETERMINISTIC = True'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    # Inicializa estado do jogo\n",
    "    def __init__(self, state, matrix_game):\n",
    "\n",
    "        # Matriz inicial de recompensas:\n",
    "        self.ini_reward_matrix = matrix_game\n",
    "\n",
    "        # Cria tabuleiro zerado com quantidade de colunas e linhas determinado\n",
    "        self.board = np.zeros([self.ini_reward_matrix.shape[0], self.ini_reward_matrix.shape[1]])\n",
    "\n",
    "        # Define locais com obstaculos\n",
    "        i=0\n",
    "        while i<= self.ini_reward_matrix.shape[0]-1:\n",
    "            j=0\n",
    "            while j <= self.ini_reward_matrix.shape[1]-1:\n",
    "                if self.ini_reward_matrix[i, j]==-100:\n",
    "                    self.board[i, j]=-100\n",
    "                j+=1\n",
    "            i+=1\n",
    "\n",
    "        # Define o estado do jogo atual. Inicialmente é igual ao parâmetro START\n",
    "        self.state = state\n",
    "\n",
    "        # Define variável de termino do jogo como False\n",
    "        self.isEnd = False\n",
    "\n",
    "    # Caso perca ou ganhe o jogo, atualiza o estado para fim\n",
    "    def isEndFunc(self, count):\n",
    "        if count == 0:\n",
    "            self.isEnd = True\n",
    "\n",
    "    # Verifica posição na matriz com base na ação\n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        if action == \"up\":\n",
    "            nxtState = (self.state[0] - 1, self.state[1])\n",
    "        elif action == \"down\":\n",
    "            nxtState = (self.state[0] + 1, self.state[1])\n",
    "        elif action == \"left\":\n",
    "            nxtState = (self.state[0], self.state[1] - 1)\n",
    "        else:\n",
    "            nxtState = (self.state[0], self.state[1] + 1)\n",
    "\n",
    "        # Se estado é legal, próximo estado, caso contrário, permanece no mesmo\n",
    "        if (nxtState[0] >= 0) and (nxtState[0] <= (self.ini_reward_matrix.shape[0] -1)):\n",
    "            if (nxtState[1] >= 0) and (nxtState[1] <= (self.ini_reward_matrix.shape[1] -1)):\n",
    "                if self.board[nxtState]!= -100:\n",
    "                    return nxtState\n",
    "        return self.state\n",
    "\n",
    "    # Mostra tabuleiro\n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, self.ini_reward_matrix.shape[0]):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, self.ini_reward_matrix.shape[1]):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -100:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                if self.ini_reward_matrix[i,j]>0:\n",
    "                    token = 't'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "    # Mostra tabuleiro\n",
    "    def showMatrix(self):\n",
    "        sns.heatmap(self.ini_reward_matrix, vmin=-1, vmax=1, annot=True)\n",
    "        \n",
    "\n",
    "# Agent of player\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, matrix_game, start, lr, gamma, d_f, exp_rate, max_exp_rate, min_exp_rate, decay_rate, max_steps):\n",
    "        \n",
    "        # Define iníco do jogo\n",
    "        self.start=start\n",
    "\n",
    "        # Matriz inicial com recompensas pré definidas:\n",
    "        self.ini_reward_matrix = matrix_game\n",
    "\n",
    "        # Contador de números maiores que 0 que ainda não foram percorridos\n",
    "        self.count = (self.ini_reward_matrix[self.ini_reward_matrix>0]).shape[1]\n",
    "\n",
    "        # Inicializa vetor de estados\n",
    "        self.states = [self.start]\n",
    "\n",
    "        # Recompensa Total\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # Inicializa vetor de ações\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "        # Inicializa classe estado\n",
    "        self.State = State(state=self.start, matrix_game=matrix_game)\n",
    "\n",
    "        # Define lr e exp_rate\n",
    "        self.lr = lr # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "        self.gamma = gamma # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "        self.d_f = d_f # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.exp_rate = exp_rate # Exploration rate\n",
    "        self.max_exp_rate = max_exp_rate # Exploration probability at start\n",
    "        self.min_exp_rate = min_exp_rate # Minimum exploration probability \n",
    "        self.decay_rate = decay_rate # Exponential decay rate for exploration prob\n",
    "        self.max_steps = max_steps # Qtd de passos até jogo ser finalizado\n",
    "\n",
    "        # Inicializa tabela Q\n",
    "            # Cria listas para cada item da coluna e da linha e combina cada item em tuplas:\n",
    "        unique_combinations=list(itertools.product(np.arange(0, self.ini_reward_matrix.shape[0], 1), np.arange(0, self.ini_reward_matrix.shape[1], 1)))\n",
    "            # Cria tabela q zerada:\n",
    "        self.qtable = pd.DataFrame(np.zeros((self.ini_reward_matrix.shape[1]*self.ini_reward_matrix.shape[0], len(self.actions))),\n",
    "                                    index=unique_combinations, columns=self.actions)\n",
    "\n",
    "        #Inicializa tabela R\n",
    "            # Cria tabela de recompensas imediatas zerada:\n",
    "        self.rtable = pd.DataFrame(np.zeros((self.ini_reward_matrix.shape[1]*self.ini_reward_matrix.shape[0], len(self.actions))),\n",
    "                                    index=unique_combinations, columns=self.actions)\n",
    "            # Enriquece tabela r com recompensas imediatas pre definidas:\n",
    "        for position in self.rtable.index:\n",
    "            #up\n",
    "            if position[0]!=0: #Se movimento for valido\n",
    "                self.rtable.loc[position, 'up'] = self.ini_reward_matrix[position[0]-1, position[1]]\n",
    "            #down\n",
    "            if position[0]<self.ini_reward_matrix.shape[0]-1: #Se movimento for valido\n",
    "                self.rtable.loc[position, 'down'] = self.ini_reward_matrix[position[0]+1, position[1]]\n",
    "            #right\n",
    "            if position[1]<self.ini_reward_matrix.shape[1]-1: #Se movimento for valido\n",
    "                self.rtable.loc[position, 'right'] = self.ini_reward_matrix[position[0], position[1]+1]\n",
    "            #left\n",
    "            if position[1]!=0: #Se movimento for valido\n",
    "                self.rtable.loc[position, 'left'] = self.ini_reward_matrix[position[0], position[1]-1]\n",
    "\n",
    "        # Cria tabela de função de valor zerada:\n",
    "        self.vtable = pd.DataFrame(np.zeros((self.ini_reward_matrix.shape[0], self.ini_reward_matrix.shape[1])))\n",
    "\n",
    "        # Vetor com recompensas. Inicializa com recompensa referente ao início do jogo\n",
    "        self.rewards = [float(self.ini_reward_matrix[self.start])]\n",
    "\n",
    "        # Listas em que serão armazenadas principais informações, para posteriormente criar arquivo csv\n",
    "        self.game_list = []\n",
    "        self.time_steps_list = []\n",
    "        self.time_list = []\n",
    "        self.states_list = []\n",
    "        self.total_rewards_list = []\n",
    "        self.rewards_list = []\n",
    "        self.vtable_list = []\n",
    "        self.qtable_list = []\n",
    "        self.state_reward_list = []\n",
    "        self.exp_rate_lst = []\n",
    "\n",
    "        # Inicializa tempo\n",
    "        self.t1=time.time()\n",
    "\n",
    "        # Seta parametro inicial de fim de jogo para False\n",
    "        self.isEnd = False\n",
    "\n",
    "    # Escolhe uma ação\n",
    "    def chooseAction(self):\n",
    "        \n",
    "        # Definindo uma semente aleatória\n",
    "        np.random.seed((os.getpid() * int(time.time_ns() // 1_000_000)) % 123456789)\n",
    "        # Inicializa variáveis\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        # Se valor aleatório for menor que taxa de exp_rate escolhida, escolhe ação aleatória\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # Escolhe ação aleatória\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        # Caso contrário, escolhe ação de acordo com estado com maior valor\n",
    "        else:\n",
    "            # Seleciona ação que maximiza tabela Q\n",
    "            # Greedy action\n",
    "            mx_nxt_reward=0\n",
    "            for a in self.actions:\n",
    "                # verifica estado com maior valor e seleciona ação correspondente\n",
    "                nxt_reward = float(self.qtable.loc[[self.State.state]][a])\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "\n",
    "        # Caso tenham duas ou mais ações empatadas com maior valor, escolhe uma aleatoriamente\n",
    "            equal_actions=[]\n",
    "            for a in self.actions:\n",
    "                # Verifica estado com maior valor e seleciona ação correspondente\n",
    "                nxt_reward = float(self.qtable.loc[[self.State.state]][a])\n",
    "                if nxt_reward == mx_nxt_reward:\n",
    "                    equal_actions.append(a)\n",
    "\n",
    "            action = np.random.choice(equal_actions)\n",
    "        return action\n",
    "\n",
    "    # Realiza ação\n",
    "    def takeAction(self, action):\n",
    "        # Altera posição atual com base na ação tomada\n",
    "        position = self.State.nxtPosition(action)\n",
    "\n",
    "        # Retorna a posição atual, depois de realizar ação\n",
    "        return State(state=position, matrix_game=self.ini_reward_matrix)\n",
    "    \n",
    "    def isEndFunc(self, steps):\n",
    "        # Caso estado atual seja maior que 0 e ainda não tenha sido percorrido, reduz contador em 1\n",
    "        if (self.State.state not in self.states[:-1]) and (self.ini_reward_matrix[self.State.state]>0):\n",
    "            self.count-=1\n",
    "\n",
    "        # Se contador é igual a zero, jogo termina\n",
    "        if self.count == 0:\n",
    "            self.isEnd = True\n",
    "\n",
    "        # Se quantidade de passos chegar a 0, jogo termina\n",
    "        if steps==self.max_steps:\n",
    "            self.isEnd = True\n",
    "    # Update Q table\n",
    "    def updateQtable(self, action):\n",
    "        # Caso já tenha passado por esse estado previamente, considera recompensa como zero\n",
    "        if self.State.nxtPosition(action) in (self.states):\n",
    "            r=0\n",
    "        else:\n",
    "            # Realiza update na tabela Q usando recompensa descontada pela quantidade de passos realizados\n",
    "            r = float(self.rtable.loc[[self.State.state]][action])*(self.d_f**(len(self.states)-1))\n",
    "\n",
    "        # Realiza update no vetor de recompensas\n",
    "        self.rewards.append(r)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        self.qtable.at[self.State.state, action] = self.qtable.loc[[self.State.state]][action]+ \\\n",
    "                                            self.lr * (r + self.gamma * np.max(self.qtable.loc[[self.State.nxtPosition(action)]].values) - \\\n",
    "                                                self.qtable.loc[[self.State.state]][action])\n",
    "\n",
    "    def calculateValuefunc(self):\n",
    "        # Cria tabela com função de valor zerada\n",
    "        self.vtable = pd.DataFrame(np.zeros((self.ini_reward_matrix.shape[0], self.ini_reward_matrix.shape[1])))\n",
    "\n",
    "        # Enriquece tabela de função de valor com valores máximos da tabela q\n",
    "        for row in self.qtable.index:\n",
    "            self.vtable.at[row[0], row[1]] = np.max(self.qtable.loc[[row]].values)\n",
    "        return self.vtable\n",
    "    \n",
    "    # Reinicia o jogo\n",
    "    def reset(self):\n",
    "        # Reinicia listas de estados percorridos e recompesas \n",
    "        self.states = [self.start]\n",
    "        self.rewards = [float(self.ini_reward_matrix[self.start])]\n",
    "        self.state_reward_dict=[]\n",
    "        self.total_reward = 0\n",
    "        #Reinicia classe de estado\n",
    "        self.State = State(state=self.start, matrix_game=self.ini_reward_matrix)\n",
    "        # Reinicia contador de números maiores que 0 que ainda não foram percorridos\n",
    "        self.count = (self.ini_reward_matrix[self.ini_reward_matrix>0]).shape[1]\n",
    "        # Reinicia tempo de cada jogo\n",
    "        self.t1 = time.time()\n",
    "\n",
    "        self.isEnd = False\n",
    "\n",
    "    # Salva informações de cada jogo em listas\n",
    "    def save_info(self, i, time):\n",
    "\n",
    "        # Armazena número do jogo em lista\n",
    "        self.game_list.append(i)\n",
    "\n",
    "        # Armazena quantidade de passos percorridos em lista\n",
    "        self.time_steps_list.append(len(self.states))\n",
    "\n",
    "        # Armazena tempo total de cada jogo em lista\n",
    "        self.time_list.append(time)\n",
    "\n",
    "        # Armazena lista de estados percorridos em lista\n",
    "        self.states_list.append(self.states)\n",
    "\n",
    "        # Armazena recompensa total do jogo em lista\n",
    "        self.total_rewards_list.append(np.sum(self.rewards))\n",
    "\n",
    "        # Armazena recompensas de cada estado em lista\n",
    "        self.rewards_list.append(self.rewards)\n",
    "\n",
    "        # Armazena vtable em lista \n",
    "        self.vtable_list.append(self.calculateValuefunc().to_dict('dict'))\n",
    "\n",
    "        # Armazena tabela Q em lista\n",
    "        self.qtable_list.append(self.qtable.to_dict('dict'))\n",
    "\n",
    "        # Armazena cada par de estado/recompensa em lista de listas\n",
    "        self.state_reward_dict = list(zip(self.states, self.rewards))\n",
    "        self.state_reward_list.append(self.state_reward_dict)\n",
    "\n",
    "        # Armazena exp_rate atual\n",
    "        self.exp_rate_lst.append(self.exp_rate)\n",
    "\n",
    "    def generate_result_file(self, rounds, steps):\n",
    "\n",
    "        # Cria dataframe e armazena listas com info de cada jogo em colunas\n",
    "        arquivo_final = pd.DataFrame()\n",
    "        arquivo_final['JOGO'] = self.game_list\n",
    "        arquivo_final['TIMESTEPS'] = self.time_steps_list\n",
    "        arquivo_final['TIME'] = self.time_list\n",
    "        arquivo_final['STATES'] = self.states_list\n",
    "        arquivo_final['REWARDS'] = self.rewards_list\n",
    "        arquivo_final['STATES_REWARDS'] = self.state_reward_list      \n",
    "        arquivo_final['TOTAL REWARDS'] = self.total_rewards_list\n",
    "        arquivo_final['Vtable'] = self.vtable_list\n",
    "        arquivo_final['Qtable'] = self.qtable_list\n",
    "        arquivo_final['exp_rate'] = self.exp_rate_lst\n",
    "\n",
    "        # date_time atual\n",
    "        date_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        # Cria arquivo com parametros do problema\n",
    "        arquivo_parametros = pd.DataFrame(data={\n",
    "                                            'parameter name': ['lr',\n",
    "                                                                'gamma',\n",
    "                                                                'd_f',\n",
    "                                                                'exp_rate',\n",
    "                                                                'max_exp_rate',\n",
    "                                                                'min_exp_rate',\n",
    "                                                                'decay_rate',\n",
    "                                                                'dimensions',\n",
    "                                                                'date',\n",
    "                                                                'rounds',\n",
    "                                                                'steps'],\n",
    "                                            'parameter': [self.lr,\n",
    "                                                            self.gamma,\n",
    "                                                            self.d_f,\n",
    "                                                            self.exp_rate,\n",
    "                                                            self.max_exp_rate,\n",
    "                                                            self.min_exp_rate,\n",
    "                                                            self.decay_rate,\n",
    "                                                            str(self.ini_reward_matrix.shape[0])+' x '+str(self.ini_reward_matrix.shape[1]),\n",
    "                                                            date_time,\n",
    "                                                            rounds,\n",
    "                                                            steps]\n",
    "})\n",
    "\n",
    "        # Exporta resultado para arquivo csv\n",
    "        with pd.ExcelWriter(f'Resultado_RL_Q_Learning/results_q_learn_dim_{self.ini_reward_matrix.shape[0]}x{self.ini_reward_matrix.shape[1]}_{date_time}.xlsx') as writer:  \n",
    "            arquivo_final.to_excel(writer, sheet_name='results')\n",
    "            arquivo_parametros.to_excel(writer, sheet_name='parameters')\n",
    "        #arquivo_final.to_excel(f'Resultado_RL_Q_Learning/results_q_learn_dim_{self.ini_reward_matrix.shape[0]}x{self.ini_reward_matrix.shape[1]}_{date_time}.xlsx', sheet_name='results')\n",
    "\n",
    "    def play(self, rounds=10):\n",
    "        # Inicializa contador de jogos\n",
    "        i = 0\n",
    "    \n",
    "        # Joga n partidas\n",
    "        while i < rounds:\n",
    "            \n",
    "            if self.isEnd==False:\n",
    "\n",
    "                # Escolhe uma ação\n",
    "                action = self.chooseAction()\n",
    "\n",
    "                #Update na tabela Q\n",
    "                self.updateQtable(action)\n",
    "\n",
    "                # appenda no vetor de estados\n",
    "                self.states.append(self.State.nxtPosition(action))\n",
    "\n",
    "                # Realiza a ação, atualiza o estado\n",
    "                self.State = self.takeAction(action)\n",
    "\n",
    "                # Define quantidade de passos como tamanho do vetor estado\n",
    "                steps=len(self.states)\n",
    "\n",
    "                # Reduz contador caso estado atual>0. Caso contador=0, jogo termina\n",
    "                self.isEndFunc(steps)\n",
    "\n",
    "            if self.isEnd==True:\n",
    "                #print('exp_rate: ',self.exp_rate)\n",
    "                #print('total reward: ', np.sum(self.rewards))\n",
    "                print('steps: ', len(self.states))\n",
    "                # Reduce epsilon (because we need less and less exploration)\n",
    "                self.exp_rate = self.min_exp_rate + (self.max_exp_rate - self.min_exp_rate)*np.exp(-self.decay_rate*i)\n",
    "\n",
    "                # Finaliza tempo de duração do jogo\n",
    "                duracao_jogo = (time.time()-self.t1)\n",
    "\n",
    "                # Printa tempo\n",
    "                print('game ' ,i, ' ended in:')\n",
    "                print(duracao_jogo)\n",
    "\n",
    "                # Salva informações em listas\n",
    "                self.save_info(i=i, time=duracao_jogo)\n",
    "\n",
    "                # Reseta jogo atual e segue para próximo jogo\n",
    "                self.reset()\n",
    "                i += 1\n",
    "        \n",
    "        # Gera Arquivo com Resultados\n",
    "        self.generate_result_file(rounds, steps)\n",
    "\n",
    "    # Mostra resultados em formato de heatmap\n",
    "    def showValues(self):\n",
    "        sns.heatmap(self.calculateValuefunc(), annot=True, fmt=\".3f\", cbar=False, cmap=sns.color_palette(\"Greens\", n_colors=100))\n",
    "        #print(self.calculateValuefunc())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ag = Agent(matrix_game=matrix_extra_simple, # Ambiente(Matriz) a ser percorrida\n",
    "#                 start=(0, 0), # Ponto de início do jogo\n",
    "#                 lr=0.001, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "#                 gamma=0.99, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "#                 d_f=0.99, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "#                 exp_rate=1, # Exploration rate\n",
    "#                 max_exp_rate=1, # Exploration probability at start\n",
    "#                 min_exp_rate=0.01, # Minimum exploration probability \n",
    "#                 decay_rate=0.001, # Exponential decay rate for exploration prob\n",
    "#                 max_steps=5000 # Qtd de passos até jogo ser finalizado\n",
    "#                 )\n",
    "\n",
    "#     ag.play(rounds=10)\n",
    "#     print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ag = Agent(matrix_game=matrix_medium, # Ambiente(Matriz) a ser percorrida\n",
    "                start=(0, 0), # Ponto de início do jogo\n",
    "                lr=0.001, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                gamma=0.9999, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                d_f=0.95, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                exp_rate=1, # Exploration rate\n",
    "                max_exp_rate=1, # Exploration probability at start\n",
    "                min_exp_rate=0.01, # Minimum exploration probability \n",
    "                decay_rate=0.005, # Exponential decay rate for exploration prob\n",
    "                max_steps=5000 # Qtd de passos até jogo ser finalizado\n",
    "                )\n",
    "\n",
    "    ag.play(rounds=10)\n",
    "    print(ag.showValues())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analisando variáveis d_f e decay rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Analisando potenciais d_f ideal para cada cenário "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada cenário, podemos ter um d_f ideal, visto na quantidade media de passos ate conclusao do jogo em cada cenário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela com d_f em função da quantidade de passos\n",
    "\n",
    "d_f_08_list = []\n",
    "d_f_09_list = []\n",
    "d_f_095_list = []\n",
    "d_f_099_list = []\n",
    "d_f_0999_list = []\n",
    "d_f_09999_list = []\n",
    "rounds = []\n",
    "\n",
    "for i in np.arange(0,10000,1):\n",
    "    rounds.append(i)\n",
    "    d_f_08_list.append(0.8**i)\n",
    "    d_f_09_list.append(0.9**i)\n",
    "    d_f_095_list.append(0.95**i)\n",
    "    d_f_099_list.append(0.99**i)\n",
    "    d_f_0999_list.append(0.999**i)\n",
    "    d_f_09999_list.append(0.9999**i)\n",
    "\n",
    "df_res_d_f = pd.DataFrame()\n",
    "df_res_d_f['rounds'] = rounds\n",
    "df_res_d_f['08 rate'] = d_f_08_list\n",
    "df_res_d_f['09 rate'] = d_f_09_list\n",
    "df_res_d_f['095 rate'] = d_f_095_list\n",
    "df_res_d_f['099 rate'] = d_f_099_list\n",
    "df_res_d_f['0999 rate'] = d_f_0999_list\n",
    "df_res_d_f['09999 rate'] = d_f_09999_list\n",
    "\n",
    "dfm = df_res_d_f.melt('rounds', var_name='cols', value_name='d_f rate')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rodando Grid Search para obtenção de melhores resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Avaliando Cenário - Matrix Extra Simple (4x3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1. Avaliando Melhor d_f para problema- Matrix Extra Simple (4x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_extra_simple_d_f(arg): \n",
    "    matrix_list = [matrix_extra_simple] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.01] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.001] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [5000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_extra_simple_d_f, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Avaliando Melhores hiperparâmetros - Matrix Extra Simple (4x3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação dos melhores hiperparâmetros dentre:\n",
    "\n",
    "* lr (learning rate) = [0.1, 0.01, 0.001]\n",
    "* gamma_list = [0.99, 0.95]\n",
    "* decay_rate_list = [0.001, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_extra_simple_param(arg): \n",
    "    matrix_list = [matrix_extra_simple] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.1, 0.01, 0.001] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99, 0.95] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.95] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.001, 0.0005] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [5000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_extra_simple_param, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Avaliando Cenário - Matrix Simple (5x5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Avaliando Melhor d_f para problema - Matrix Simple (5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_simple_d_f(arg): \n",
    "    matrix_list = [matrix_simple] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.01] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.8] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.001] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [10000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_simple_d_f, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Avaliando Melhores hiperparâmetros - Matrix Simple (5x5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação dos melhores hiperparâmetros dentre:\n",
    "\n",
    "* lr (learning rate) = [0.1, 0.01, 0.001]\n",
    "* gamma_list = [0.99, 0.95]\n",
    "* decay_rate_list = [0.001, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_simple_param(arg): \n",
    "    matrix_list = [matrix_simple] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.001] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99, 0.95] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.8] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.001, 0.0005] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [10000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_simple_param, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Avaliando Cenário - Matrix Complex (9x13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. Avaliando Melhor d_f para problema - Matrix Complex (9x13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_complex_d_f(arg): \n",
    "    matrix_list = [matrix_complex] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.01] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.99, 0.999,0.9999] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability\n",
    "    decay_rate_list = [0.001] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [10000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_complex_d_f, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Avaliando Cenário - Matrix Medium (8x8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Avaliando Melhor d_f para problema - Matrix Medium (8x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_medium_d_f(arg): \n",
    "    matrix_list = [matrix_medium] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.01] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.99, 0.999, 0.9999] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.001] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [10000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_medium_d_f, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Avaliando Melhores hiperparâmetros - Matrix Medium (8x8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação dos melhores hiperparâmetros dentre:\n",
    "\n",
    "* lr (learning rate) = [0.01, 0.001]\n",
    "* gamma_list = [0.99, 0.95]\n",
    "* decay_rate_list = [0.001, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_medium_param(arg): \n",
    "    matrix_list = [matrix_medium] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.01] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.99, 0.97] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.99] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.001, 0.0005] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [10000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_medium_param, range(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2. Avaliando Melhores hiperparâmetros - Matrix Complex (9x13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando resultado com mesmos hiperparâmetros finais de matrix 8x8:\n",
    "\n",
    "* lr (learning rate) = [0.001]\n",
    "* gamma_list = [0.99]\n",
    "* decay_rate_list = [0.0005]\n",
    "* d_f = [0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando por combinações dos parâmetros 20x\n",
    "def grid_matrix_complex_param(arg):\n",
    "    matrix_list = [matrix_complex] # Ambiente(Matriz) a ser percorrida\n",
    "    lr_list = [0.001] # [0.01, 0.001, 0.0001] Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "    gamma_list = [0.97] # [0.95, 0.99] Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "    d_f_list = [0.99] # [0.8, 0.9, 0.95, 0.99, 0.999, 0.9999] d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "    exp_rate_list = [1] # Exploration rate\n",
    "    min_exp_rate_list = [0.01] # Minimum exploration probability \n",
    "    decay_rate_list = [0.0005] # [0.001, 0.005] Exponential decay rate for exploration prob\n",
    "    max_steps_list = [10000] # Qtd de passos até jogo ser finalizado\n",
    "    rounds_list = [8000] # Qtd de jogos\n",
    "\n",
    "    for matrix in matrix_list:\n",
    "        for lr in lr_list:\n",
    "            for gamma in gamma_list:\n",
    "                for d_f in d_f_list:\n",
    "                    for exp_rate in exp_rate_list:\n",
    "                        for min_exp_rate in min_exp_rate_list:\n",
    "                            for decay_rate in decay_rate_list:\n",
    "                                for max_steps in max_steps_list:\n",
    "                                    for rounds in rounds_list:\n",
    "                                        if __name__ == \"__main__\":\n",
    "                                            ag = Agent(matrix_game=matrix, # Ambiente(Matriz) a ser percorrida\n",
    "                                                        start=(0, 0), # Ponto de início do jogo\n",
    "                                                        lr=lr, # Learning Rate: Grau de atualização de peso para estado/ação atual\n",
    "                                                        gamma=gamma, # Gamma: Define o quanto a proxima recompensa na matriz Q é relevante para atualização do estado/ação atual\n",
    "                                                        d_f=d_f, # d_f: grau de redução de recompensas futuras baseadas em relação a quantidade de passos\n",
    "                                                        exp_rate=exp_rate, # Exploration rate\n",
    "                                                        max_exp_rate=exp_rate, # Exploration probability at start\n",
    "                                                        min_exp_rate=min_exp_rate, # Minimum exploration probability \n",
    "                                                        decay_rate=decay_rate, # Exponential decay rate for exploration prob\n",
    "                                                        max_steps=max_steps # Qtd de passos até jogo ser finalizado\n",
    "                                                        )\n",
    "                                            ag.play(rounds=rounds)\n",
    "                                            print(f\"donne with configuration: {matrix.shape[0]}x{matrix.shape[1]}_lr_{lr}_gamma_{gamma}_d_f_{d_f}_exp_rate_{exp_rate}_max_exp_rate_{exp_rate}_min_exp_rate_{min_exp_rate}_decay_rate_{decay_rate}_max_steps_{max_steps}_rounds_{rounds}_n_{arg}\")\n",
    "                                            print(ag.showValues())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # execute tasks in order\n",
    "       pool.map(grid_matrix_complex_param, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquivos de resultado\n",
    "\n",
    "# # Arquivo 1\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_17-13-07.xlsx\")\n",
    "# df.to_pickle(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_17-13-07_results.pickle\", compression='xz')\n",
    "\n",
    "# # Arquivo 2\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_21-44-41.xlsx\")\n",
    "# df.to_pickle('Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_21-44-41_results.pickle', compression='xz')\n",
    "\n",
    "# # Arquivo 3\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_00-44-20.xlsx\")\n",
    "# df.to_pickle('Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_00-44-20_results.pickle', compression='xz')\n",
    "\n",
    "# # Arquivo 4\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_04-55-06.xlsx\")\n",
    "# df.to_pickle('Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_04-55-06_results.pickle', compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquivos de parametros\n",
    "\n",
    "# # Arquivo 1\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_17-13-07.xlsx\", sheet_name='parameters')\n",
    "# df.to_pickle(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_17-13-07_param.pickle\", compression='xz')\n",
    "\n",
    "# # Arquivo 2\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_21-44-41.xlsx\", sheet_name='parameters')\n",
    "# df.to_pickle('Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_21-44-41_param.pickle', compression='xz')\n",
    "\n",
    "# # Arquivo 3\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_00-44-20.xlsx\", sheet_name='parameters')\n",
    "# df.to_pickle('Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_00-44-20_param.pickle', compression='xz')\n",
    "\n",
    "# # Arquivo 4\n",
    "# df = pd.read_excel(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_04-55-06.xlsx\", sheet_name='parameters')\n",
    "# df.to_pickle('Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-23_04-55-06_param.pickle', compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(\"Resultado_RL_Q_Learning/03_Matriz Complex_9x13/01_Analise GridSearch/results_q_learn_dim_9x13_2023-06-22_17-13-07.pickle\", compression = 'xz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rl_path_covering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
